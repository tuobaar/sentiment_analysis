{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4140,"sourceType":"datasetVersion","datasetId":2477},{"sourceId":9819040,"sourceType":"datasetVersion","datasetId":6020345}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import libraries\nimport pandas as pd\nimport joblib\nimport re\nimport nltk\nimport os\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\n\n# Download NLTK resources if needed\n# nltk.download('wordnet', download_dir=\"/kaggle/working/\")\n# nltk.download('stopwords', download_dir=\"/kaggle/working/\")\n# nltk.download('punkt', download_dir=\"/kaggle/working/\")\n\nnltk.download('wordnet')\nnltk.download('omw-1.4')  # For wordnet language support if needed\nnltk.download('stopwords')\n# nltk.data.path.append(\"/kaggle/working/nltk\")\n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\n!unzip /usr/share/nltk_data/corpora/omw-1.4.zip -d /usr/share/nltk_data/corpora/\n\nstop_words = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()\nstemmer = PorterStemmer()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-07T14:46:25.261872Z","iopub.execute_input":"2024-11-07T14:46:25.263232Z","iopub.status.idle":"2024-11-07T14:46:32.108562Z","shell.execute_reply.started":"2024-11-07T14:46:25.263165Z","shell.execute_reply":"2024-11-07T14:46:32.106864Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the Sentiment140 dataset\ndf = pd.DataFrame()\n\ndata_path = '/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv'\ntry:\n    df = pd.read_csv(data_path, encoding='latin-1', header=None)\n    df.columns = ['polarity', 'id', 'date', 'query', 'user', 'text']  # Rename columns\n\n    # Filter out neutral polarity\n    df = df[df['polarity'] != 2]\n\n    # Map polarity to binary labels: 0 = negative, 1 = positive\n    df['polarity'] = df['polarity'].map({0: 0, 4: 1})\n\n    # Drop unnecessary columns\n    df = df[['polarity', 'text']]\n\n    print(df.head())  # Preview dataset\n\nexcept Exception as e:\n    print(f\"Error loading dataset: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T14:46:56.322103Z","iopub.execute_input":"2024-11-07T14:46:56.323645Z","iopub.status.idle":"2024-11-07T14:47:04.019593Z","shell.execute_reply.started":"2024-11-07T14:46:56.323574Z","shell.execute_reply":"2024-11-07T14:47:04.018451Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T14:47:09.484232Z","iopub.execute_input":"2024-11-07T14:47:09.484745Z","iopub.status.idle":"2024-11-07T14:47:09.701588Z","shell.execute_reply.started":"2024-11-07T14:47:09.484687Z","shell.execute_reply":"2024-11-07T14:47:09.700345Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preprocessing text\n\n# Fetch emoji and sentiment from csv\ndef load_emoji_sentiment(csv_file):\n    emoji_df = pd.read_csv(csv_file)\n    emoji_sentiment_dict = dict(zip(emoji_df['Emoji'], emoji_df['Sentiment']))\n    return emoji_sentiment_dict\n\n# Load the emoji sentiment dictionary at runtime\nemoji_sentiment_dict = load_emoji_sentiment('/kaggle/input/emoji-with-sentiments/emoji_sentiment.csv')\n\ndef replace_emojis(text):\n    for emoji, replacement in emoji_sentiment_dict.items():\n        text = text.replace(emoji, replacement)\n    return text\n\ndef remove_short_words(text):\n    return ' '.join([word for word in text.split() if len(word) >= 2])\n\ndef remove_stopwords(text):\n    return ' '.join([word for word in text.split() if word not in stop_words])\n\ndef apply_stemming(text):\n    return ' '.join([stemmer.stem(word) for word in text.split()])\n\ndef apply_lemmatization(text):\n    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n\n\ndef clean_text(text):\n    text = text.lower()  # Convert text to lowercase\n    text = replace_emojis(text)  # Replace emojis with their meaningful text\n    text = re.sub(r'http\\S+', 'url', text)  # Replace urls with 'url'\n    text = re.sub(r'\\b\\w*@\\w*\\.\\w*\\b', 'email', text)  # Replace email addresses with 'email'\n    text = re.sub(r'@\\w+', 'user', text)  # Replace user-mentions with 'user'\n    text = re.sub(r'#', '', text)  # Remove hashtag symbols\n    text = re.sub(r'\\d+', '', text)  # Remove numbers\n    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)  # Remove repeated characters\n    text = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text)  # Remove consecutive duplicate words\n\n    # Handle repeated words without spaces\n    text = re.sub(r'(\\b\\w+)\\1+', r'\\1', text)\n\n    # Reduce consecutive duplicates\n    text = re.sub(r'(\\b\\w+)(\\1)+', r'\\1', text)  # Reduce repeated words\n    \n    text = remove_short_words(text)  # Remove short words\n    text = remove_stopwords(text)   # Remove stopwords\n    text = apply_stemming(text)  # Apply stemming\n    text = apply_lemmatization(text)  # Optional: apply both or just one\n    \n    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespace\n    \n    return text\n\n\ndf['text'] = df['text'].apply(clean_text)\n\ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the cleaned dataset to CSV\ndf.to_csv('cleaned_stemmed_lemmatized_sentiment140.csv', index=False, encoding='utf-8')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the cleaned dataset to Parquet\ndf.to_parquet('cleaned_stemmed_lemmatized_sentiment140.parquet', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the DataFrame using joblib\njoblib.dump(df, 'cleaned_stemmed_lemmatized_sentiment140.joblib')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}